{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a66d8ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from scipy.sparse import hstack, csr_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import classification_report, roc_auc_score, confusion_matrix\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "27f4e37c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure paths and feature settings\n",
    "PROJECT_ROOT = Path.cwd().parent\n",
    "CLEAN_DIR = PROJECT_ROOT / 'preprocessed_data'\n",
    "OUT_DIR = PROJECT_ROOT / 'feature_matrices'\n",
    "OUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# Feature settings\n",
    "TFIDF_MAX_FEAT = 10000\n",
    "TFIDF_NGRAMS = (1, 2)\n",
    "SVD_COMPONENTS = 300\n",
    "NUM_COLS = ['goal', 'pledged', 'usd_pledged', 'duration_days', 'pledged_to_goal', 'backers']\n",
    "CAT_COLS = ['main_category', 'currency', 'state']\n",
    "\n",
    "# Model settings\n",
    "RANDOM_STATE = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "489d5673",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4 CSV files in /home/lance/CODEWRLD/ICS4A/ML/crowdfundingproject/preprocessed_data\n",
      "Combined dataset shape: (710411, 14)\n",
      "Columns: ['name', 'blurb', 'category', 'main_category', 'currency', 'deadline', 'goal', 'launched', 'pledged', 'state', 'backers', 'country', 'usd_pledged', 'pledged_to_goal_ratio']\n"
     ]
    }
   ],
   "source": [
    "# Load and align datasets\n",
    "csv_files = glob.glob(os.path.join(CLEAN_DIR, '*.csv'))\n",
    "if len(csv_files) < 1:\n",
    "    raise ValueError(f\"No CSV files found in {CLEAN_DIR}\")\n",
    "\n",
    "print(f\"Found {len(csv_files)} CSV files in {CLEAN_DIR}\")\n",
    "\n",
    "# Load datasets\n",
    "dfs = []\n",
    "for file in csv_files:\n",
    "    df = pd.read_csv(file)\n",
    "    # Rename columns for consistency\n",
    "    rename_dict = {\n",
    "        'amt.pledged': 'pledged',\n",
    "        'title': 'name',\n",
    "        'num.backers': 'backers',\n",
    "        'end.time': 'deadline',\n",
    "        'percentage.funded': 'pledged_to_goal_ratio',\n",
    "        'usd_pledged': 'usd_pledged'\n",
    "    }\n",
    "    df.rename(columns=rename_dict, inplace=True)\n",
    "    # Ensure common columns\n",
    "    common_cols = [\n",
    "        'name', 'blurb', 'category', 'main_category', 'currency', 'deadline',\n",
    "        'goal', 'launched', 'pledged', 'state', 'backers', 'country',\n",
    "        'usd_pledged', 'pledged_to_goal_ratio'\n",
    "    ]\n",
    "    # Add missing columns\n",
    "    for col in common_cols:\n",
    "        if col not in df.columns:\n",
    "            if col in ['name', 'blurb']:\n",
    "                df[col] = ''\n",
    "            elif col in ['category', 'main_category', 'currency', 'country', 'state']:\n",
    "                df[col] = 'unknown'\n",
    "            else:\n",
    "                df[col] = np.nan\n",
    "    df = df[common_cols]\n",
    "    dfs.append(df)\n",
    "\n",
    "# Concatenate datasets\n",
    "data = pd.concat(dfs, ignore_index=True)\n",
    "print(\"Combined dataset shape:\", data.shape)\n",
    "print(\"Columns:\", data.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "16a828f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column data types:\n",
      " name                      object\n",
      "blurb                     object\n",
      "category                  object\n",
      "main_category             object\n",
      "currency                  object\n",
      "deadline                  object\n",
      "goal                     float64\n",
      "launched                  object\n",
      "pledged                  float64\n",
      "state                     object\n",
      "backers                   object\n",
      "country                   object\n",
      "usd_pledged               object\n",
      "pledged_to_goal_ratio    float64\n",
      "dtype: object\n",
      "Fraud label distribution:\n",
      " fraud_label\n",
      "0    702938\n",
      "1      7473\n",
      "Name: count, dtype: int64\n",
      "Missing values:\n",
      " name                     0\n",
      "blurb                    0\n",
      "category                 0\n",
      "main_category            0\n",
      "currency                 0\n",
      "deadline                 0\n",
      "goal                     0\n",
      "launched                 0\n",
      "pledged                  0\n",
      "state                    0\n",
      "backers                  0\n",
      "country                  0\n",
      "usd_pledged              0\n",
      "pledged_to_goal_ratio    0\n",
      "duration_days            0\n",
      "pledged_to_goal          0\n",
      "fraud_label              0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Handle missing data and engineer features\n",
    "# Inspect column data types\n",
    "print(\"Column data types:\\n\", data.dtypes)\n",
    "\n",
    "# Convert numerical columns to float, coercing invalid values to NaN\n",
    "num_cols_to_fix = ['goal', 'pledged', 'backers', 'usd_pledged']\n",
    "for col in num_cols_to_fix:\n",
    "    data[col] = pd.to_numeric(data[col], errors='coerce')\n",
    "    if data[col].dtype not in ['float64', 'int64']:\n",
    "        print(f\"Warning: Column {col} still contains non-numeric values after coercion.\")\n",
    "        print(data[col].head())\n",
    "\n",
    "# Missing data\n",
    "data['name'] = data['name'].fillna('')\n",
    "data['blurb'] = data['blurb'].fillna('')\n",
    "data['category'] = data['category'].fillna('unknown')\n",
    "data['main_category'] = data['main_category'].fillna('unknown')\n",
    "data['currency'] = data['currency'].fillna('unknown')\n",
    "data['country'] = data['country'].fillna('unknown')\n",
    "data['state'] = data['state'].fillna('unknown')\n",
    "data['deadline'] = pd.to_datetime(data['deadline'], errors='coerce').fillna(pd.Timestamp('2020-01-01'))\n",
    "data['launched'] = pd.to_datetime(data['launched'], errors='coerce').fillna(pd.Timestamp('2020-01-01'))\n",
    "data[num_cols_to_fix] = data[num_cols_to_fix].fillna(data[num_cols_to_fix].median())\n",
    "data['pledged_to_goal_ratio'] = data['pledged_to_goal_ratio'].fillna(data['pledged'] / data['goal'].replace(0, 1))\n",
    "\n",
    "# Engineer features\n",
    "data['duration_days'] = (data['deadline'] - data['launched']).dt.days.clip(lower=1)\n",
    "data['pledged_to_goal'] = data['pledged'] / data['goal'].replace(0, 1)\n",
    "\n",
    "# Engineer fraud label\n",
    "data['fraud_label'] = (\n",
    "    ((data['goal'] > 500000) & (data['backers'] < 20)) |  # High goal, low backers\n",
    "    ((data['state'] == 'failed') & (data['pledged_to_goal'] < 0.1) & (data['duration_days'] < 15))  # Failed, low funding, short duration\n",
    ").astype(int)\n",
    "print(\"Fraud label distribution:\\n\", data['fraud_label'].value_counts())\n",
    "print(\"Missing values:\\n\", data.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f97271ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize feature extractors\n",
    "tfidf = TfidfVectorizer(max_features=TFIDF_MAX_FEAT, ngram_range=TFIDF_NGRAMS, stop_words='english')\n",
    "svd = TruncatedSVD(n_components=SVD_COMPONENTS, random_state=RANDOM_STATE)\n",
    "scaler = StandardScaler()\n",
    "ohe = OneHotEncoder(handle_unknown='ignore', sparse_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "176184e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text column: blurb\n",
      "Numeric columns: ['goal', 'pledged', 'usd_pledged', 'duration_days', 'pledged_to_goal', 'backers']\n",
      "Categorical columns: ['main_category', 'currency', 'state']\n",
      "SVD components shape: (710411, 300)\n",
      "Numeric matrix shape: (710411, 6)\n",
      "Categorical matrix shape: (710411, 846)\n",
      "Combined feature matrix shape: (710411, 1152)\n"
     ]
    }
   ],
   "source": [
    "# Extract features\n",
    "# Identify columns\n",
    "text_col = 'blurb' if data['blurb'].str.strip().ne('').any() else 'name'\n",
    "num_cols = ['goal', 'pledged', 'usd_pledged', 'duration_days', 'pledged_to_goal', 'backers']\n",
    "cat_cols = ['main_category', 'currency', 'state']\n",
    "\n",
    "print(\"Text column:\", text_col)\n",
    "print(\"Numeric columns:\", num_cols)\n",
    "print(\"Categorical columns:\", cat_cols)\n",
    "\n",
    "# Text features: TF-IDF + SVD\n",
    "try:\n",
    "    X_tfidf = tfidf.fit_transform(data[text_col])\n",
    "    X_svd = svd.fit_transform(X_tfidf)\n",
    "    print(\"SVD components shape:\", X_svd.shape)\n",
    "except Exception as e:\n",
    "    print(f\"Error in TF-IDF/SVD: {e}\")\n",
    "    X_svd = np.zeros((len(data), SVD_COMPONENTS))\n",
    "\n",
    "# Numerical features: Scale\n",
    "try:\n",
    "    if num_cols and not data[num_cols].isnull().all().any():\n",
    "        X_num = scaler.fit_transform(data[num_cols])\n",
    "        print(\"Numeric matrix shape:\", X_num.shape)\n",
    "    else:\n",
    "        X_num = np.empty((len(data), 0))\n",
    "        print(\"No valid numeric columns.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error in scaling numerical features: {e}\")\n",
    "    X_num = np.empty((len(data), 0))\n",
    "\n",
    "# Categorical features: One-hot encode\n",
    "try:\n",
    "    if cat_cols:\n",
    "        X_cat = ohe.fit_transform(data[cat_cols])\n",
    "        print(\"Categorical matrix shape:\", X_cat.shape)\n",
    "    else:\n",
    "        X_cat = csr_matrix((len(data), 0))\n",
    "        print(\"No categorical columns.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error in one-hot encoding: {e}\")\n",
    "    X_cat = csr_matrix((len(data), 0))\n",
    "\n",
    "# Combine features\n",
    "try:\n",
    "    X_all = hstack([csr_matrix(X_svd), X_cat, csr_matrix(X_num)]).tocsr()\n",
    "    print(\"Combined feature matrix shape:\", X_all.shape)\n",
    "except Exception as e:\n",
    "    print(f\"Error combining features: {e}\")\n",
    "    X_all = csr_matrix((len(data), 0))\n",
    "\n",
    "# Save feature matrix\n",
    "np.save(OUT_DIR / 'X_all.npy', X_all.toarray())  # Save as dense for compatibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b6eb6e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for modeling\n",
    "X = X_all\n",
    "y = data['fraud_label']\n",
    "try:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=RANDOM_STATE, stratify=y)\n",
    "    print(\"Training set shape:\", X_train.shape)\n",
    "    print(\"Test set shape:\", X_test.shape)\n",
    "except Exception as e:\n",
    "    print(f\"Error in train-test split: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cff521a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for modeling\n",
    "X = X_all\n",
    "y = data['fraud_label']\n",
    "try:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=RANDOM_STATE, stratify=y)\n",
    "    print(\"Training set shape:\", X_train.shape)\n",
    "    print(\"Test set shape:\", X_test.shape)\n",
    "except Exception as e:\n",
    "    print(f\"Error in train-test split: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "614b34d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle class imbalance\n",
    "smote = SMOTE(random_state=RANDOM_STATE)\n",
    "try:\n",
    "    X_train_bal, y_train_bal = smote.fit_resample(X_train, y_train)\n",
    "    print(\"Balanced training label distribution:\\n\", pd.Series(y_train_bal).value_counts())\n",
    "except Exception as e:\n",
    "    print(f\"SMOTE failed: {e}. Using original training data.\")\n",
    "    X_train_bal, y_train_bal = X_train, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ccf245",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Random Forest\n",
    "rf_model = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    class_weight='balanced',\n",
    "    max_depth=10,\n",
    "    min_samples_split=5,\n",
    "    random_state=RANDOM_STATE,\n",
    "    n_jobs=-1\n",
    ")\n",
    "try:\n",
    "    rf_model.fit(X_train_bal, y_train_bal)\n",
    "    # Evaluate\n",
    "    y_pred_rf = rf_model.predict(X_test)\n",
    "    y_pred_proba_rf = rf_model.predict_proba(X_test)[:, 1]\n",
    "    print(\"\\nRandom Forest Classification Report:\\n\")\n",
    "    print(classification_report(y_test, y_pred_rf, target_names=['Legitimate', 'Fraudulent']))\n",
    "    print(\"Random Forest ROC-AUC:\", roc_auc_score(y_test, y_pred_proba_rf))\n",
    "\n",
    "    # Confusion Matrix\n",
    "    cm_rf = confusion_matrix(y_test, y_pred_rf)\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    sns.heatmap(cm_rf, annot=True, fmt='d', cmap='Blues', xticklabels=['Legitimate', 'Fraudulent'], yticklabels=['Legitimate', 'Fraudulent'])\n",
    "    plt.title('Random Forest Confusion Matrix')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.show()\n",
    "except Exception as e:\n",
    "    print(f\"Error in Random Forest training/evaluation: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99e2ce6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train XGBoost\n",
    "scale_pos_weight = max(1.0, (y_train == 0).sum() / (y_train == 1).sum())\n",
    "xgb_model = XGBClassifier(\n",
    "    max_depth=6,\n",
    "    learning_rate=0.1,\n",
    "    n_estimators=100,\n",
    "    scale_pos_weight=scale_pos_weight,\n",
    "    reg_alpha=1.0,  # L1 regularization\n",
    "    reg_lambda=1.0,  # L2 regularization\n",
    "    random_state=RANDOM_STATE,\n",
    "    n_jobs=-1\n",
    ")\n",
    "try:\n",
    "    xgb_model.fit(X_train_bal, y_train_bal)\n",
    "    # Evaluate\n",
    "    y_pred_xgb = xgb_model.predict(X_test)\n",
    "    y_pred_proba_xgb = xgb_model.predict_proba(X_test)[:, 1]\n",
    "    print(\"\\nXGBoost Classification Report:\\n\")\n",
    "    print(classification_report(y_test, y_pred_xgb, target_names=['Legitimate', 'Fraudulent']))\n",
    "    print(\"XGBoost ROC-AUC:\", roc_auc_score(y_test, y_pred_proba_xgb))\n",
    "\n",
    "    # Confusion Matrix\n",
    "    cm_xgb = confusion_matrix(y_test, y_pred_xgb)\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    sns.heatmap(cm_xgb, annot=True, fmt='d', cmap='Blues', xticklabels=['Legitimate', 'Fraudulent'], yticklabels=['Legitimate', 'Fraudulent'])\n",
    "    plt.title('XGBoost Confusion Matrix')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.show()\n",
    "except Exception as e:\n",
    "    print(f\"Error in XGBoost training/evaluation: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af2807b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Logistic Regression\n",
    "lr_model = LogisticRegression(\n",
    "    class_weight='balanced',\n",
    "    penalty='l1',\n",
    "    solver='liblinear',\n",
    "    max_iter=1000,\n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "try:\n",
    "    lr_model.fit(X_train_bal, y_train_bal)\n",
    "    # Evaluate\n",
    "    y_pred_lr = lr_model.predict(X_test)\n",
    "    y_pred_proba_lr = lr_model.predict_proba(X_test)[:, 1]\n",
    "    print(\"\\nLogistic Regression Classification Report:\\n\")\n",
    "    print(classification_report(y_test, y_pred_lr, target_names=['Legitimate', 'Fraudulent']))\n",
    "    print(\"Logistic Regression ROC-AUC:\", roc_auc_score(y_test, y_pred_proba_lr))\n",
    "\n",
    "    # Confusion Matrix\n",
    "    cm_lr = confusion_matrix(y_test, y_pred_lr)\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    sns.heatmap(cm_lr, annot=True, fmt='d', cmap='Blues', xticklabels=['Legitimate', 'Fraudulent'], yticklabels=['Legitimate', 'Fraudulent'])\n",
    "    plt.title('Logistic Regression Confusion Matrix')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.show()\n",
    "except Exception as e:\n",
    "    print(f\"Error in Logistic Regression training/evaluation: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d4a4bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare models and save\n",
    "models = {'Random Forest': rf_model, 'XGBoost': xgb_model, 'Logistic Regression': lr_model}\n",
    "results = {}\n",
    "for name, model in models.items():\n",
    "    try:\n",
    "        y_pred = model.predict(X_test)\n",
    "        results[name] = {\n",
    "            'F1-Score (Fraud)': classification_report(y_test, y_pred, output_dict=True)['Fraudulent']['f1-score'],\n",
    "            'ROC-AUC': roc_auc_score(y_test, model.predict_proba(X_test)[:, 1])\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Error evaluating {name}: {e}\")\n",
    "        results[name] = {'F1-Score (Fraud)': 0, 'ROC-AUC': 0}\n",
    "results_df = pd.DataFrame(results).T\n",
    "print(\"\\nModel Comparison:\\n\", results_df)\n",
    "\n",
    "# Save models\n",
    "import joblib\n",
    "joblib.dump(rf_model, OUT_DIR / 'rf_fraud_model.pkl')\n",
    "joblib.dump(xgb_model, OUT_DIR / 'xgb_fraud_model.pkl')\n",
    "joblib.dump(lr_model, OUT_DIR / 'lr_fraud_model.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
